{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0516f04-fa02-4d21-8d58-b44af83c2a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import gc\n",
    "\n",
    "dataset = load_dataset(\"wmt20_mlqe_task1\", \"en-de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00d14d6a-84df-4edc-8cbb-622dc10d4aff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['inputs', 'targets'],\n",
       "        num_rows: 7000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['inputs', 'targets'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['inputs', 'targets'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_languages(examples):\n",
    "    targets = [ex[\"de\"] for ex in examples[\"translation\"]]\n",
    "    inputs = [ex[\"en\"] for ex in examples[\"translation\"]]\n",
    "    return {\"inputs\": inputs, \"targets\": targets}\n",
    "\n",
    "dataset = dataset.map(extract_languages, batched=True, remove_columns=['segid', 'translation', 'scores', 'mean', 'z_scores', 'z_mean', 'model_score', 'doc_id', 'nmt_output', 'word_probas'])\n",
    "dataset  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fdd6332-f2db-4624-83da-c4d78c6de093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training data is: 7000\n",
      "Length of validation data is: 1000\n",
      "Length of testing data is: 1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training = dataset[\"train\"]\n",
    "validation = dataset[\"validation\"]\n",
    "testing = dataset[\"test\"]\n",
    "\n",
    "print(f\"Length of training data is: {len(training)}\")\n",
    "print(f\"Length of validation data is: {len(validation)}\")\n",
    "print(f\"Length of testing data is: {len(testing)}\")\n",
    "\n",
    "del dataset\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f55b05ab-7b12-42d8-9914-6f1b145215e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Memory used after loading the model: 423.18 MB\n",
      "GPU Memory used after loading the model: 95.62 MB\n"
     ]
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "import psutil\n",
    "\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "   bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "\n",
    "# Function to get memory usage in megabytes\n",
    "def memory_usage_mb():\n",
    "    return psutil.Process().memory_info().rss / (1024 ** 2)\n",
    "\n",
    "# Function to get GPU memory usage in MB\n",
    "def gpu_memory_usage_mb():\n",
    "    torch.cuda.synchronize()  # Wait for all operations on the GPU to complete\n",
    "    return torch.cuda.memory_allocated() / (1024 ** 2)\n",
    "\n",
    "\n",
    "# Measure memory before loading the model\n",
    "memory_before = memory_usage_mb()\n",
    "gpu_memory_before = gpu_memory_usage_mb()\n",
    "\n",
    "tokenizer_q = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "model_q = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\", quantization_config=nf4_config)\n",
    "\n",
    "# Measure memory after loading the model\n",
    "memory_after = memory_usage_mb()\n",
    "gpu_memory_after = gpu_memory_usage_mb()\n",
    "\n",
    "# Calculate and print the difference in memory usage\n",
    "print(f\"System Memory used after loading the model: {memory_after - memory_before:.2f} MB\")\n",
    "print(f\"GPU Memory used after loading the model: {gpu_memory_after - gpu_memory_before:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "630ca406-dc80-4252-a425-4e50db78fe87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model_q.gradient_checkpointing_enable()\n",
    "model_q = prepare_model_for_kbit_training(model_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1f1b6d1-7887-413a-85c1-dcce751d6c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96e8a4ff-87fc-4a01-b2f0-72e2cb3f6489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 294912 || all params: 45072896 || trainable%: 0.6543000920109504\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8, \n",
    "    lora_alpha=32,  \n",
    "    lora_dropout=0.05, \n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "model_q = get_peft_model(model_q, config)\n",
    "print_trainable_parameters(model_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e94e853b-8868-4d41-ad50-e66662f8b229",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_lang = \"inputs\"\n",
    "target_lang = \"targets\"\n",
    "prefix = \"translate English eo German: \"\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + example for example in examples[source_lang]]\n",
    "    targets = [example for example in examples[target_lang]]\n",
    "    model_inputs = tokenizer_q(inputs, max_length=128, truncation=True)\n",
    "\n",
    "    with tokenizer_q.as_target_tokenizer():\n",
    "        labels = tokenizer_q(targets, max_length=128, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_training_data = training.map(preprocess_function, batched=True)\n",
    "validation_training_data = validation.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5987473-0ca7-4a30-8b9f-9f6f64110c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-21 17:13:01.425194: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-21 17:13:01.453880: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-21 17:13:01.947333: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer_q, model=model_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "122cba9c-94f5-48df-a8dc-c18eb7a14651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5256' max='5256' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5256/5256 15:22, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.986200</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.963200</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.965400</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.953100</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.938600</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.951800</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.940400</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.940400</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.941100</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.942300</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.938700</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5256, training_loss=0.9515452101894709, metrics={'train_runtime': 922.5745, 'train_samples_per_second': 91.05, 'train_steps_per_second': 5.697, 'total_flos': 1074345608478720.0, 'train_loss': 0.9515452101894709, 'epoch': 12.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results_2\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=12,\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model_q,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_training_data,\n",
    "    eval_dataset=validation_training_data,\n",
    "    tokenizer=tokenizer_q,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e94f287-0368-42a1-bb33-0e025caf79b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./en_de_model_qlora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4e6e072-810b-4513-972e-8ac8fddb703a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score for the quantized fine tuned t5 model is: 46.83953379552536\n",
      "Execution time: 367.23499274253845 seconds\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "import sacrebleu\n",
    "import time\n",
    "\n",
    "\n",
    "def translator_pipeline(model_name, tokenizer_name):\n",
    "    translator = pipeline(\"translation_en_to_de\", model=model_name, tokenizer=tokenizer_name)\n",
    "    return translator\n",
    "\n",
    "def translate_and_evaluate(test_dataset):\n",
    "    translations = []\n",
    "    references = []\n",
    "\n",
    "    for item in test_dataset:\n",
    "        # Translate each sentence\n",
    "        english_sentence = item['inputs']\n",
    "        german_translation = translator(english_sentence, max_length=128, truncation=True)[0]['translation_text']\n",
    "\n",
    "        # Append the result and the reference translation\n",
    "        translations.append(german_translation)\n",
    "        references.append(item['targets'])\n",
    "\n",
    "    return translations, references\n",
    "\n",
    "\n",
    "# Function to calculate BLEU score using SacreBLEU\n",
    "def calculate_bleu_score(translations, references):\n",
    "    bleu = sacrebleu.corpus_bleu(translations, [references])\n",
    "    return bleu.score\n",
    "\n",
    "\n",
    "# Load the trained model\n",
    "model_path = \"./en_de_model_qlora\"  \n",
    "our_model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "our_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "translator = translator_pipeline(our_model, our_tokenizer)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "translations, references = translate_and_evaluate(testing)\n",
    "\n",
    "# Calculate the BLEU score\n",
    "bleu_score = calculate_bleu_score(translations, references)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"BLEU score for the quantized fine tuned t5 model is: {bleu_score}\")\n",
    "print(f\"Execution time: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1834c01a-f437-4cdd-b8f1-4f5fdd5ece77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
